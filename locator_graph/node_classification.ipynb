{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "ObMapfNgc6xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6FxfkpD5_amh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/drive/MyDrive/fault_loc_dataset/RNN10.py"
      ],
      "metadata": {
        "id": "ZhS92f5Ge8f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/fault_loc_dataset/"
      ],
      "metadata": {
        "id": "Mg4eMcg_evhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/fault_loc_dataset/fault_loc_dataset.csv')"
      ],
      "metadata": {
        "id": "vFK9mwco_lNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "QdB0Lg9KAA6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/MyDrive/fault_loc_dataset/\""
      ],
      "metadata": {
        "id": "fW_FSsDm_5cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['full_path'] = base_dir + df['Filename']"
      ],
      "metadata": {
        "id": "6H8A3GHo_68g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_buggy_files = df[df['buggy'] == True]['full_path'].tolist()\n",
        "all_right_models = df[df['buggy'] == False]['full_path'].tolist()"
      ],
      "metadata": {
        "id": "HvjuZ3wtAF4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_right_models"
      ],
      "metadata": {
        "id": "oxuJpUIyAU6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat /content/drive/MyDrive/fault_loc_dataset/GRU5.py"
      ],
      "metadata": {
        "id": "JvoGcYkifgea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "model_code = \"\"\"\n",
        "model = Sequential()\n",
        "model.add(GRU(256 , return_sequences=True))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(64 ,  activation = 'relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Define default values for variables\n",
        "default_values = {\n",
        "    'vocab_size': 10000,\n",
        "    'embedding_dim': 100,\n",
        "    'max_length': 80,\n",
        "    'gru_dim': 64,\n",
        "    'dense_dim': 32,\n",
        "    'max_features': 10000\n",
        "}\n",
        "\n",
        "# Define default attributes for different layer types\n",
        "default_attributes = {\n",
        "    'Embedding': {\n",
        "        'input_dim': None,\n",
        "        'output_dim': None,\n",
        "        'input_length': None\n",
        "    },\n",
        "    'GRU': {\n",
        "        'units': None,\n",
        "        'activation': 'tanh',\n",
        "        'recurrent_activation': 'sigmoid',\n",
        "        'use_bias': True,\n",
        "        'kernel_initializer': 'glorot_uniform',\n",
        "        'recurrent_initializer': 'orthogonal',\n",
        "        'bias_initializer': 'zeros',\n",
        "        'unit_forget_bias': True,\n",
        "        'kernel_regularizer': None,\n",
        "        'recurrent_regularizer': None,\n",
        "        'bias_regularizer': None,\n",
        "        'activity_regularizer': None,\n",
        "        'kernel_constraint': None,\n",
        "        'recurrent_constraint': None,\n",
        "        'bias_constraint': None,\n",
        "        'dropout': 0.0,\n",
        "        'recurrent_dropout': 0.0,\n",
        "        'implementation': 1,\n",
        "        'return_sequences': False,\n",
        "        'return_state': False,\n",
        "        'go_backwards': False,\n",
        "        'stateful': False,\n",
        "        'unroll': False,\n",
        "        'time_major': False,\n",
        "        'reset_after': True\n",
        "    },\n",
        "    'Dense': {\n",
        "        'units': None,\n",
        "        'activation': None,\n",
        "        'use_bias': True,\n",
        "        'kernel_initializer': 'glorot_uniform',\n",
        "        'bias_initializer': 'zeros',\n",
        "        'kernel_regularizer': None,\n",
        "        'bias_regularizer': None,\n",
        "        'activity_regularizer': None,\n",
        "        'kernel_constraint': None,\n",
        "        'bias_constraint': None,\n",
        "        'lora_rank': None\n",
        "    },\n",
        "    'Dropout': {\n",
        "        'rate': None,\n",
        "        'noise_shape': None,\n",
        "        'seed': None\n",
        "    },\n",
        "    'LSTM': {\n",
        "        'units': None,\n",
        "        'activation': 'tanh',\n",
        "        'recurrent_activation': 'sigmoid',\n",
        "        'use_bias': True,\n",
        "        'kernel_initializer': 'glorot_uniform',\n",
        "        'recurrent_initializer': 'orthogonal',\n",
        "        'bias_initializer': 'zeros',\n",
        "        'unit_forget_bias': True,\n",
        "        'kernel_regularizer': None,\n",
        "        'recurrent_regularizer': None,\n",
        "        'bias_regularizer': None,\n",
        "        'activity_regularizer': None,\n",
        "        'kernel_constraint': None,\n",
        "        'recurrent_constraint': None,\n",
        "        'bias_constraint': None,\n",
        "        'dropout': 0.0,\n",
        "        'recurrent_dropout': 0.0,\n",
        "        'implementation': 2,\n",
        "        'return_sequences': False,\n",
        "        'return_state': False,\n",
        "        'go_backwards': False,\n",
        "        'stateful': False,\n",
        "        'unroll': False,\n",
        "        'time_major': False\n",
        "    },\n",
        "    'Conv2D': {\n",
        "        'filters': None,\n",
        "        'kernel_size': None,\n",
        "        'strides': (1, 1),\n",
        "        'padding': 'valid',\n",
        "        'data_format': None,\n",
        "        'dilation_rate': (1, 1),\n",
        "        'activation': None,\n",
        "        'use_bias': True,\n",
        "        'kernel_initializer': 'glorot_uniform',\n",
        "        'bias_initializer': 'zeros',\n",
        "        'kernel_regularizer': None,\n",
        "        'bias_regularizer': None,\n",
        "        'activity_regularizer': None,\n",
        "        'kernel_constraint': None,\n",
        "        'bias_constraint': None\n",
        "    },\n",
        "    'Conv1D': {\n",
        "        'filters': None,\n",
        "        'kernel_size': None,\n",
        "        'strides': 1,\n",
        "        'padding': 'valid',\n",
        "        'data_format': None,\n",
        "        'dilation_rate': 1,\n",
        "        'activation': None,\n",
        "        'use_bias': True,\n",
        "        'kernel_initializer': 'glorot_uniform',\n",
        "        'bias_initializer': 'zeros',\n",
        "        'kernel_regularizer': None,\n",
        "        'bias_regularizer': None,\n",
        "        'activity_regularizer': None,\n",
        "        'kernel_constraint': None,\n",
        "        'bias_constraint': None\n",
        "    },\n",
        "    'MaxPooling2D': {\n",
        "        'pool_size': (2, 2),\n",
        "        'strides': None,\n",
        "        'padding': 'valid',\n",
        "        'data_format': None\n",
        "    },\n",
        "    'MaxPooling1D': {\n",
        "        'pool_size': 2,\n",
        "        'strides': None,\n",
        "        'padding': 'valid'\n",
        "    },\n",
        "    'Flatten': {}\n",
        "}\n",
        "\n",
        "class ModelGraphBuilder(ast.NodeVisitor):\n",
        "    def __init__(self):\n",
        "        self.graph = []\n",
        "\n",
        "    def visit_Call(self, node):\n",
        "        if isinstance(node.func, ast.Attribute) and node.func.attr == 'add':\n",
        "            layer = node.args[0]\n",
        "            if isinstance(layer.func, ast.Name) and layer.func.id == 'Bidirectional':\n",
        "                wrapped_layer = layer.args[0]\n",
        "                self.add_layer(wrapped_layer, bidirectional=True)\n",
        "            else:\n",
        "                self.add_layer(layer)\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def get_value(self, node):\n",
        "        if isinstance(node, ast.Name):\n",
        "            return default_values.get(node.id, node.id)\n",
        "        return ast.literal_eval(node)\n",
        "\n",
        "    def get_layer_type(self, layer_func):\n",
        "        if isinstance(layer_func, ast.Attribute):\n",
        "            return layer_func.attr\n",
        "        elif isinstance(layer_func, ast.Name):\n",
        "            return layer_func.id\n",
        "        return None\n",
        "\n",
        "    def add_layer(self, layer, bidirectional=False):\n",
        "        layer_type = self.get_layer_type(layer.func)\n",
        "        if layer_type is None:\n",
        "            return\n",
        "\n",
        "        layer_attributes = default_attributes.get(layer_type, {}).copy()\n",
        "\n",
        "        # Handle positional arguments\n",
        "        for i, arg in enumerate(layer.args):\n",
        "            keys = list(layer_attributes.keys())\n",
        "            if i < len(keys):\n",
        "                layer_attributes[keys[i]] = self.get_value(arg)\n",
        "\n",
        "        # Handle keyword arguments\n",
        "        for kw in layer.keywords:\n",
        "            layer_attributes[kw.arg] = self.get_value(kw.value)\n",
        "\n",
        "        if bidirectional:\n",
        "            self.graph.append((f'Bidirectional_{layer_type}_forward', layer_attributes.copy()))\n",
        "            self.graph.append((f'Bidirectional_{layer_type}_backward', layer_attributes.copy()))\n",
        "        else:\n",
        "            self.graph.append((layer_type, layer_attributes))\n",
        "\n",
        "# Parse the code\n",
        "tree = ast.parse(model_code)\n",
        "\n",
        "# Build the graph\n",
        "builder = ModelGraphBuilder()\n",
        "builder.visit(tree)\n",
        "\n",
        "# Print the graph with default attributes\n",
        "for node in builder.graph:\n",
        "    print(node)\n"
      ],
      "metadata": {
        "id": "32SxRhLQuGl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "all_right_graphs = []\n",
        "all_right_labels = []\n",
        "for path in tqdm(all_right_models):\n",
        "    with open(path, 'r') as file:\n",
        "        content = file.read()\n",
        "    try:\n",
        "        # Parse the code\n",
        "        tree = ast.parse(content)\n",
        "\n",
        "        # Build the graph\n",
        "        builder = ModelGraphBuilder()\n",
        "        builder.visit(tree)\n",
        "        all_right_graphs.append(builder.graph)\n",
        "        all_right_labels.append(0)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing file {path}: {e}\")\n",
        "\n",
        "all_fault_graphs = []\n",
        "all_fault_labels = []\n",
        "for path in tqdm(all_buggy_files):\n",
        "    with open(path, 'r') as file:\n",
        "        content = file.read()\n",
        "        label = path.split('/')[-1].split('.')[0].split('_')[1]\n",
        "        label = 'b1' if label in ['b10', 'b11', 'b12', 'b13', 'b14', 'b15', 'b16', 'b17', 'b18', 'b19'] else label\n",
        "        label = label.replace('b', '')\n",
        "        label = int(label)\n",
        "    try:\n",
        "        # Parse the code\n",
        "        tree = ast.parse(content)\n",
        "\n",
        "        # Build the graph\n",
        "        builder = ModelGraphBuilder()\n",
        "        builder.visit(tree)\n",
        "        all_fault_graphs.append(builder.graph)\n",
        "        all_fault_labels.append(label)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing file {path}: {e}\")\n"
      ],
      "metadata": {
        "id": "o-b5s2QoB6ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_right_labels = [0] * len(all_right_graphs)\n",
        "all_fault_labels = [1] * len(all_fault_graphs)"
      ],
      "metadata": {
        "id": "OwWc5biIKUpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_class_labels = all_right_labels + all_fault_labels"
      ],
      "metadata": {
        "id": "8iL7TA-iZLAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_graphs = all_right_graphs + all_fault_graphs\n",
        "all_labels = all_right_labels + all_fault_labels"
      ],
      "metadata": {
        "id": "sucHTOgJLACq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_graphs), len(all_labels), len(all_class_labels)"
      ],
      "metadata": {
        "id": "1WFUxCBVZ7D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "from collections import Counter\n",
        "\n",
        "def balance_dataset(graphs, labels):\n",
        "    \"\"\"\n",
        "    Balances the dataset by oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "    Args:\n",
        "        graphs (list): List of graphs.\n",
        "        labels (list): List of labels corresponding to the graphs.\n",
        "\n",
        "    Returns:\n",
        "        balanced_graphs (list): Balanced list of graphs.\n",
        "        balanced_labels (list): Balanced list of labels.\n",
        "    \"\"\"\n",
        "    # Separate by class\n",
        "    class_indices = {}\n",
        "    for index, label in enumerate(labels):\n",
        "        if label not in class_indices:\n",
        "            class_indices[label] = []\n",
        "        class_indices[label].append(index)\n",
        "\n",
        "    # Determine the maximum class size\n",
        "    max_count = max(len(indices) for indices in class_indices.values())\n",
        "\n",
        "    balanced_graphs = []\n",
        "    balanced_labels = []\n",
        "\n",
        "    for label, indices in class_indices.items():\n",
        "        class_graphs = [graphs[i] for i in indices]\n",
        "        class_labels = [labels[i] for i in indices]\n",
        "\n",
        "        # Oversample minority class\n",
        "        if len(class_graphs) < max_count:\n",
        "            class_graphs_oversampled, class_labels_oversampled = resample(\n",
        "                class_graphs,\n",
        "                class_labels,\n",
        "                replace=True,\n",
        "                n_samples=max_count,\n",
        "                random_state=42\n",
        "            )\n",
        "            balanced_graphs.extend(class_graphs_oversampled)\n",
        "            balanced_labels.extend(class_labels_oversampled)\n",
        "        else:\n",
        "            balanced_graphs.extend(class_graphs)\n",
        "            balanced_labels.extend(class_labels)\n",
        "\n",
        "    return balanced_graphs, balanced_labels\n",
        "\n",
        "# Example usage\n",
        "balanced_graphs, balanced_labels = balance_dataset(all_graphs, all_class_labels)\n",
        "\n",
        "# Now you can proceed with further processing or batching\n"
      ],
      "metadata": {
        "id": "-wNe3VsDLgT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(balanced_labels)"
      ],
      "metadata": {
        "id": "QMcm-_Mkb8Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import torch\n",
        "from torch_geometric.data import Data, Batch\n",
        "\n",
        "def preprocess_graphs(graphs, labels, possible_activations):\n",
        "    \"\"\"\n",
        "    Preprocesses multiple graphs into node features and edge index for GNN input.\n",
        "\n",
        "    Args:\n",
        "        graphs (list of list): List of graphs, where each graph is a list of tuples (layer, attributes).\n",
        "        labels (list): List of labels corresponding to each graph.\n",
        "        possible_activations (list): List of possible activation functions for one-hot encoding.\n",
        "\n",
        "    Returns:\n",
        "        Batch: PyTorch Geometric Batch object containing all processed graphs.\n",
        "    \"\"\"\n",
        "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "    try:\n",
        "        encoder.fit(np.array(possible_activations).reshape(-1, 1))\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting encoder: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Attributes to keep\n",
        "    attributes_to_keep = [\n",
        "        'units', 'activation', 'recurrent_activation', 'kernel_regularizer', 'recurrent_regularizer',\n",
        "        'bias_regularizer', 'activity_regularizer', 'recurrent_dropout', 'dropout'\n",
        "    ]\n",
        "\n",
        "    data_list = []\n",
        "    max_node_feature_length = 0\n",
        "\n",
        "    for graph_index, (graph, label) in enumerate(zip(graphs, labels)):\n",
        "        node_features = []\n",
        "        edge_index = []\n",
        "\n",
        "        try:\n",
        "            for layer, attributes in graph:\n",
        "                encoded_attributes = {}\n",
        "\n",
        "                if 'activation' in attributes:\n",
        "                    activation = attributes.get('activation', 'None')\n",
        "                    if activation not in possible_activations:\n",
        "                        print(f\"Warning: Activation function '{activation}' not in possible_activations\")\n",
        "                        activation = 'None'\n",
        "                    encoded_activation = encoder.transform([[activation]]).flatten().tolist()\n",
        "                    encoded_attributes['activation'] = encoded_activation\n",
        "\n",
        "                if 'recurrent_activation' in attributes:\n",
        "                    recurrent_activation = attributes.get('recurrent_activation', 'None')\n",
        "                    if recurrent_activation not in possible_activations:\n",
        "                        print(f\"Warning: Recurrent activation function '{recurrent_activation}' not in possible_activations\")\n",
        "                        recurrent_activation = 'None'\n",
        "                    encoded_recurrent_activation = encoder.transform([[recurrent_activation]]).flatten().tolist()\n",
        "                    encoded_attributes['recurrent_activation'] = encoded_recurrent_activation\n",
        "\n",
        "                for key in attributes_to_keep:\n",
        "                    if key in attributes and key not in ['activation', 'recurrent_activation']:\n",
        "                        value = attributes.get(key, 0)\n",
        "                        if value is None:\n",
        "                            value = 0\n",
        "                        encoded_attributes[key] = value\n",
        "\n",
        "                combined_features = []\n",
        "                if 'activation' in encoded_attributes:\n",
        "                    combined_features.extend(encoded_attributes['activation'])\n",
        "                if 'recurrent_activation' in encoded_attributes:\n",
        "                    combined_features.extend(encoded_attributes['recurrent_activation'])\n",
        "\n",
        "                for key in attributes_to_keep:\n",
        "                    if key in encoded_attributes and key not in ['activation', 'recurrent_activation']:\n",
        "                        combined_features.append(encoded_attributes[key])\n",
        "\n",
        "                node_features.append(combined_features)\n",
        "                max_node_feature_length = max(max_node_feature_length, len(combined_features))\n",
        "\n",
        "            if not node_features:\n",
        "                print(f\"Warning: Skipping empty node_features for graph {graph_index}\")\n",
        "                continue\n",
        "\n",
        "            # Ensure all node features have the same length\n",
        "            node_features = [f + [0] * (max_node_feature_length - len(f)) for f in node_features]\n",
        "            node_features = np.array(node_features, dtype=float)\n",
        "            x = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "            # Create edge index\n",
        "            num_nodes = len(node_features)\n",
        "            if num_nodes > 1:\n",
        "                edge_index = torch.tensor(\n",
        "                    [[i, i + 1] for i in range(num_nodes - 1)] +\n",
        "                    [[i + 1, i] for i in range(num_nodes - 1)], dtype=torch.long\n",
        "                ).t().contiguous()\n",
        "            else:\n",
        "                edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty edge_index for single-node graphs\n",
        "\n",
        "            # Convert label to tensor\n",
        "            y = torch.tensor([label], dtype=torch.long)\n",
        "\n",
        "            data = Data(x=x, edge_index=edge_index, y=y)\n",
        "            data_list.append(data)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing graph {graph_index}: {e}\")\n",
        "            continue\n",
        "\n",
        "    try:\n",
        "        if not data_list:\n",
        "            print(\"Warning: No valid graphs to create a batch.\")\n",
        "            return None\n",
        "\n",
        "        # Create batch from list of graphs\n",
        "        batch = Batch.from_data_list(data_list)\n",
        "\n",
        "        # Debugging: Print shapes of tensors in data_list\n",
        "        for i, data in enumerate(data_list):\n",
        "            print(f\"Graph {i} - x shape: {data.x.shape}, edge_index shape: {data.edge_index.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating batch: {e}\")\n",
        "        raise\n",
        "\n",
        "    return batch\n"
      ],
      "metadata": {
        "id": "Zn9c-Kr06Zeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the possible activations\n",
        "possible_activations = [ None,\n",
        "    'relu', 'sigmoid', 'tanh', 'softmax', 'softplus', 'softsign',\n",
        "    'selu', 'elu', 'exponential', 'linear', 'None',\n",
        " 'gelu', 'selu', 'softmax', 'softplus', 'softsign',               'leaky_relu', 'silu', 'hard_silu', 'mish',\n",
        "                         'hard_sigmoid', 'relu6'\n",
        "]\n",
        "\n",
        "\n",
        "# Prepare data\n",
        "batch = preprocess_graphs(balanced_graphs, balanced_labels, possible_activations)"
      ],
      "metadata": {
        "id": "dnJ2zo9F_OrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GNNNodeClassification(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_classes):\n",
        "        super(GNNNodeClassification, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, 16)\n",
        "        self.conv3 = GCNConv(16, 16)\n",
        "        self.fc = torch.nn.Linear(16, num_classes)  # `num_classes` corresponds to the number of classes for node classification\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Output layer for node classification (no global pooling)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)  # log_softmax over the class dimension"
      ],
      "metadata": {
        "id": "K5A7m0UF5q2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch.num_nodes"
      ],
      "metadata": {
        "id": "aG2QPWlQ665X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch.y = node_labels"
      ],
      "metadata": {
        "id": "3ce7iwAn5ebG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import DataLoader"
      ],
      "metadata": {
        "id": "IFOoTUy66Jmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GNNNodeClassification(num_node_features=batch.num_features, num_classes=2)  # Binary classification\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def calculate_accuracy(pred, labels):\n",
        "    pred_labels = pred.argmax(dim=1)  # Get the predicted labels\n",
        "    correct = (pred_labels == labels).sum().item()  # Count correct predictions\n",
        "    accuracy = correct / labels.size(0)  # Calculate accuracy\n",
        "    return accuracy\n",
        "\n",
        "def train(batch, epochs=100):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "        out = model(batch)  # Forward pass\n",
        "        loss = loss_fn(out, batch.y)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update parameters\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = calculate_accuracy(out, batch.y)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "train(batch)"
      ],
      "metadata": {
        "id": "DMOt_OKd6ChM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bb4z72MO9615"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}