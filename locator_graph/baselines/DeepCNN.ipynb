{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSNx4BsGeu-S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizerFast, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "class CodeDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        with open(item['full_path'], 'r') as file:\n",
        "            content = file.read()\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            content,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        inputs = {key: value.squeeze(0) for key, value in inputs.items()}\n",
        "        inputs['labels'] = torch.tensor(1 if item['buggy'] else 0)\n",
        "        return inputs\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "train_dataset = CodeDataset(train_df, tokenizer)\n",
        "val_dataset = CodeDataset(val_df, tokenizer)\n",
        "test_dataset = CodeDataset(test_df, tokenizer)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.layernorm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.layernorm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.dropout1 = nn.Dropout(rate)\n",
        "        self.dropout2 = nn.Dropout(rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.att(x, x, x)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(nn.Module):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = nn.Embedding(maxlen, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "class SimpleTransformerModel(nn.Module):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, num_heads, ff_dim, num_classes):\n",
        "        super(SimpleTransformerModel, self).__init__()\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc1 = nn.Linear(embed_dim, 20)\n",
        "        self.fc2 = nn.Linear(20, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer_block(x)\n",
        "        x = x.permute(1, 2, 0)\n",
        "        x = self.global_avg_pool(x).squeeze(-1)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc2(x)\n",
        "        return logits\n",
        "\n",
        "# Parameters\n",
        "maxlen = 512\n",
        "vocab_size = tokenizer.vocab_size\n",
        "embed_dim = 128\n",
        "num_heads = 8\n",
        "ff_dim = 512\n",
        "num_classes = 2\n",
        "batch_size = 8\n",
        "epochs = 10\n",
        "learning_rate = 2e-5\n",
        "adam_epsilon = 1e-8\n",
        "warmup_steps = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model, optimizer, scheduler\n",
        "model = SimpleTransformerModel(maxlen, vocab_size, embed_dim, num_heads, ff_dim, num_classes).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
        "total_steps = len(train_dataset) * epochs // batch_size\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    preds = preds.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    auc = roc_auc_score(labels, preds)\n",
        "    return acc, precision, recall, f1, auc\n",
        "\n",
        "def train(model, loader, optimizer, scheduler):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = {key: value.to(device) for key, value in batch.items() if key != 'labels'}\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(inputs['input_ids'])\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            inputs = {key: value.to(device) for key, value in batch.items() if key != 'labels'}\n",
        "            label = batch['labels'].to(device)\n",
        "            output = model(inputs['input_ids'])\n",
        "            preds.extend(output.cpu().numpy())\n",
        "            labels.extend(label.cpu().numpy())\n",
        "    acc, precision, recall, f1, auc = compute_metrics(np.array(preds), np.array(labels))\n",
        "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, scheduler)\n",
        "    val_metrics = evaluate(model, val_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    print(f\"Train Loss: {train_loss}\")\n",
        "    print(f\"Validation Metrics: {val_metrics}\")\n",
        "\n",
        "test_metrics = evaluate(model, test_loader)\n",
        "print(\"Test Metrics:\", test_metrics)\n"
      ]
    }
  ]
}