{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVoQT8K7YDtB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "model.to(device)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "class CodeDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        with open(item['full_path'], 'r') as file:\n",
        "            content = file.read()\n",
        "        input_ids = self.tokenizer.encode(content, truncation=True, padding='max_length', max_length=512)\n",
        "        labels = torch.tensor(1 if item['buggy'] else 0)\n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids),\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "dataset = CodeDataset(df)\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, PLBartForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['buggy'])\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42, stratify=train_df['buggy'])  # 0.25 * 0.8 = 0.2\n",
        "\n",
        "# Separate the majority and minority classes\n",
        "train_majority = train_df[train_df['buggy'] == 0]\n",
        "train_minority = train_df[train_df['buggy'] == 1]\n",
        "\n",
        "# Upsample the minority class\n",
        "train_minority_upsampled = resample(train_minority,\n",
        "                                    replace=True,    # sample with replacement\n",
        "                                    n_samples=len(train_majority),  # to match majority class\n",
        "                                    random_state=42)  # reproducible results\n",
        "\n",
        "# Combine majority class with upsampled minority class\n",
        "train_df = pd.concat([train_majority, train_minority_upsampled])\n",
        "\n",
        "class CodeDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer=True):\n",
        "        self.data = dataframe\n",
        "        if tokenizer:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        labels = torch.tensor(1 if item['buggy'] else 0)\n",
        "        with open(item['full_path'], 'r') as file:\n",
        "            content = file.read()\n",
        "        if self.tokenizer:\n",
        "            input_ids = self.tokenizer.encode(content, truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "            return {\n",
        "                'input_ids': torch.tensor(input_ids),\n",
        "                'labels': labels\n",
        "            }\n",
        "        else:\n",
        "            return content, labels\n",
        "\n",
        "train_dataset = CodeDataset(train_df, tokenizer=True)\n",
        "val_dataset = CodeDataset(val_df, tokenizer=True)\n",
        "test_dataset = CodeDataset(test_df, tokenizer=True)\n",
        "\n",
        "\n",
        "\n",
        "# Load the tokenizer and model\n",
        "model = PLBartForSequenceClassification.from_pretrained(\"uclanlp/plbart-base\", num_labels=2)\n",
        "# Define metrics computation\n",
        "def compute_metrics(p):\n",
        "    # print(p.predictions)\n",
        "    preds = p.predictions[0].argmax(-1)\n",
        "    probs = p.predictions[0][:, 1]\n",
        "    labels = p.label_ids\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    auc = roc_auc_score(labels, probs)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': auc,\n",
        "    }\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "val_metrics = trainer.evaluate(eval_dataset=val_dataset)\n",
        "print(\"Validation metrics:\", val_metrics)\n",
        "\n",
        "test_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(\"Test metrics:\", test_metrics)\n",
        "\n",
        "trainer.train()\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
